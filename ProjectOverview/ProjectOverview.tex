\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{amsmath,amssymb,amsthm}

\title{Descriptions of Projects for Summer 2020 CSST}
\author{HanQin Cai, Yuchen Lou and Daniel McKenzie}
\date{2020}

\DeclareMathOperator{\sign}{\mathrm{sign}}
\DeclareMathOperator{\bfg}{\mathbf{g}}

\begin{document}
\maketitle

\begin{abstract}
Let's outline the scope and goals of the various projects we are pursuing this summer. At the moment, I'm focusing on the projects that have not yet been clearly defined. So, I won't go into too much detail on Tree and Block Sparse ZORO.
\end{abstract}



\section{Structured Sparsity Extensions to ZORO}

\begin{itemize}
	\item As discussed, we want to extend ZORO to be able to deal with gradients that exhibit structured sparsity. The two cases we have examined are tree-sparse and block-sparse. \\
	\item We note that in both cases, the structure has to be known {\em a priori}. Clearly, this is unrealistic in some applications. Thus, it is of interest to see whether the tree structure can be {\em learned}. Specifically, this would mean the following:
	\begin{itemize}
		\item For the first $T$ iterations, use regular ZORO. Compute the gradient estimators $\hat{\mathbf{g}}_{k}$ and let $S_{k} = \text{supp}(\hat{\mathbf{g}}_k)$.
		\item Assume that there exists an underlying tree $\mathcal{T}$, such that all gradients for this problem are $\mathcal{T}$-sparse. For now lets assume that all gradients should be $s$-tree-sparse ({\em i.e.} have $s$ non-zero entries that form a subtree of $\mathcal{T}$).
		\item Observe that each $S_{k}$ is the set of vertices of an $s$-vertex subtree of $\mathcal{T}$. Interestingly, observe that $S_{k}$ alone does not tell us any parent/child relationships.
		\item Let's write $\mathcal{T} = (\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ represents the vertices and $\mathcal{E}$ represents the edges. Lets assume that $T$ is large enough that $\cup_{k=1}^{T} S_k = \mathcal{V}$. 
		\item Here is the interesting question: Can we infer $\mathcal{E}$ from $\{S_1,\ldots, S_{T}\}$? This seems hard, but one could exploit the tree structure. For example if $S_{k_1}$ and $S_{k_2}$ differ by one index, call it $i$, then we can infer that $i$ cannot be a parent to any of the vertices in $S_{k_1}$ or $S_{k_2}$. 
	\end{itemize}
	Suppose that this problem can be solved, and now let $T$ be the minimum number such that the problem is solvable with $S_1,\ldots, S_{T}$. Then one can make an adaptive algorithm that runs regular ZORO for $T$ iterations, then reconstructs $\mathcal{T}$, and then switches to tree-sparse ZORO for the remainder of the optimization process.\\
	
\item Possible applications. We (i.e. HanQin and Daniel) have some expertise in adversarial attacks on classification algorithms. This is quite a trendy application of zeroth-order optimization, so I think it could be a nice way to apply structure-aware ZORO. Three possible ways to do this (and don't worry if this doesn't all make sense to you at this stage):
	\begin{itemize}
		\item Attacks on decision trees. This paper: \cite{cheng2018query} details a way to do this. They use a generic zeroth-order algorithm to do the attack, but it can probably be sped up by using a tree-sparsity-aware zeroth-order algorithm. The downside is that one would need to know the tree, which would mean it's only partially black-box.
		\item Attacks that are sparse in the wavelet domain on Convolutional Neural Networks for image classification.
		\item Block-sparse attacks on Convolutional Neural Networks for image classification.This paper: \cite{xu2018structured} has some details on this, but I haven't read it closely.
	\end{itemize} 

\end{itemize}

\section{Exploiting Block Sparsity for High Dimensional ZORO}

\begin{itemize}
	\item Suppose that we are solving $\text{minimize}_{x\in\mathbb{R}^{d}} f(x)$ where $d$ is extremely large.
	\item ZORO generates $z_{i}\in\mathbb{R}^{d}$ with $z_{i,j} = \pm 1$ and then uses finite differences to get $y_{i} \approx z_{i}^{\top}\mathbf{g}_k$. 
	\item As we've discussed, the $z_i$ are then stacked into a matrix $Z\in\mathbb{R}^{m\times d}$. By construction, this matrix is now full ({\em i.e.} not sparse. When $d$ is large this can require too much memory, and operations with this matrix might be too slow.
	\item So, my idea is the following:
		\begin{itemize}
			\item Divide $\{1,\ldots, d\}$ into $D$ blocks of size $d/D$. Lets label the blocks $\mathbf{b}_{1},\ldots, \mathbf{b}_{D}$. Note that $\mathbf{b}_{a} = \{a*(d/D)+1,\ldots, (a+1)*(d/D)\}$. 
			\item Now, only generate $z_{i}$ that are {\em supported on only one block}. That is, $\text{supp}(z_i) = \mathbf{b}_{a_i}$ for some $a_i$. There are several ways one could do it. Given $i$ one could choose the $a_i$ uniformly at random. Alternatively, at the $k$-th iteration of the gradient descent part of ZORO, one could fix a block $\mathbf{b}_{a_{k}}$, and then choose all the $z_i$ at this iteration to have support $\mathbf{b}_{a_k}$. This turns ZORO into a sort of block coordinate descent algorithm, which could be very interesting to analyze.
			\item That's where this paper: \cite{chun2020uniform} comes in. Hopefully we can use their analysis to establish when the matrix $Z$ has the RIP, and hence use the analysis in the ZORO paper to bound $\|\mathbf{g}_k - \hat{\mathbf{g}}_k\|$. 
		
		\end{itemize}

\end{itemize}

\section{Block Coordinate descent for ZORO}
This section generalizes the previous one. Suppose again that we are solving $\text{minimize}_{x\in\mathbb{R}^{d}}$ where $d$ is extremely large. Let's assume that we have a block-diagonal sensing matrix:
$$
Z = \left[\begin{matrix} Z_1 & \ldots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \ldots & Z_{C}\end{matrix} \right] \in \mathbb{R}^{m\times d}
$$
where $Z_{c}\in\mathbb{R}^{m_{c}\times d_{c}}$. 

\begin{enumerate}
	\item The blocks can either be randomly assigned or can correspond to user-specified parts of the problem. Let's assume that they are equally sized, so that $m_{c} = m/C$ and $d_{c} = d/c$. 
	
	\item Observe that the problem $y = Z\mathbf{g}$ decouples into problems $y_{c} = Z_{c} \mathbf{g}_{c}$ for $c = 1,\ldots, C$. 
	
	\item Let us assume that $\|\mathbf{g}_{c}\|_{0} \leq 1.1 s/C$, where $s = \|\mathbf{g}\|_{0}$. We can guarantee this with high probability, if the blocks are random. Alternatively we can make this a requirement that the user-specified blocks must satisfy. 
	
	\item In the random block case, one should have $s = \alpha d$ where $\alpha\in (0,1)$ and $C$ a fixed constant, independent of $d$. Then:
	$$
	\mathbb{P} \left[\|\mathbf{g}_{c}\|_{0} \leq  f(\alpha,d) s/C \right] \leq 1 - \frac{1}{d} \text{ with } f(\alpha, d) \to 0 \text{ as } d \to \infty 
	$$
	from Hoeffding's inequality or the Chernoff Bounds. 
	\item Note that each $Z_{c}$ will satisfy the $1.1s/C$ RIP with high probability, as long as $m/C$ is sufficiently large. Thus CoSaMP will succesfully solve each problem:
	\begin{equation}
	\hat{\mathbf{g}}^{(c)} = \text{argmin}_{\mathbf{g}^{'}\in\mathbb{R}^{d_{c}}} \left\{ \|y_{c} - Z_{c}\mathbf{g}^{'}\|_{2} \text{ subject to: } \|\mathbf{g}^{'}\|_{0} \leq s_{c}\right\}
	\label{eq:SubProblem}
	\end{equation}
 Note further that one could do this in parallel. 
 
 \item More interestingly, one could solve \eqref{eq:SubProblem} for only one $c$. This would lead to an {\em inexact block coordinate descent scheme}: $x_{k+1} = x_{k} - \alpha \hat{\bfg}^{(c)}$, where we are abusing notation slightly by considering $\hat{\bfg}^{(c)}$ to be both a vector in $\mathbb{R}^{d_c}$ and a vector in $\mathbb{R}^{n}$ padded with zeros. 
 
 \item For $x_{k+1} = x_{k} - \alpha \hat{\bfg}^{(c)}$, we must have $\alpha \leq \frac{1}{L^{(c)}}$ where $\|\nabla_{c}f(x) - \nabla_{c}f(y) \|\leq L^{(c)}\|x- y\|$.  The point is $L = \min_{c} L^{(c)}$, so coordinate descent can, in principle, take larger steps (and therefore converge faster).

\item The paper \cite{tappenden2016inexact} studies this exact problem, and provides rates of convergence. 

\end{enumerate}

\section{Variance Reduction for ZORO}

Yuchen's note sent on Friday 12th June summarizes this part quite nicely. The only thing I would add is that I think we can further exploit the structure of the error terms, $e_i = z_i^{\top}\nabla^{2}f(x)z_i$. In particular, one can use the Hanson-Wright inequality (see pg. 139 of this book: \cite{vershynin2018high}) to get bounds of the form:
$$
\mathbb{P}\left[\left| z_i^{\top}\nabla^{2}f(x)z_i - \mathbb{E}\left[z_i^{\top}\nabla^{2}f(x)z_i\right]\right| \geq t \right] \leq e^{-ct} 
$$
Of course one would then also need to bound the difference between the true mean, $\mathbb{E}\left[z_i^{\top}\nabla^{2}f(x)z_i\right]$, and the sample mean:
$$
\bar{e} = \frac{1}{m}\sum_i e_i = \frac{1}{m} \sum_i z_i^{\top}\nabla^{2}f(x)z_i 
$$
so getting sharper bounds than those given by Popoviciu's inequality might not be so straightforward.
\bibliographystyle{alpha}
\bibliography{CSST2020.bib}
\end{document}