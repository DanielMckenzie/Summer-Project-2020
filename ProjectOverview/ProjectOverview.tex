\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{amsmath,amssymb,amsthm}

\title{Descriptions of Projects for Summer 2020 CSST}
\author{HanQin Cai, Yuchen Lou and Daniel McKenzie}
\date{2020}

\DeclareMathOperator{\sign}{\mathrm{sign}}
\DeclareMathOperator{\bfg}{\mathbf{g}}

\begin{document}
\maketitle

\begin{abstract}
Let's outline the scope and goals of the various projects we are pursuing this summer. At the moment, I'm focusing on the projects that have not yet been clearly defined. So, I won't go into too much detail on Tree and Block Sparse ZORO.
\end{abstract}



\section{Structured Sparsity Extensions to ZORO}

\begin{itemize}
	\item As discussed, we want to extend ZORO to be able to deal with gradients that exhibit structured sparsity. The two cases we have examined are tree-sparse and block-sparse. \\
	\item We note that in both cases, the structure has to be known {\em a priori}. Clearly, this is unrealistic in some applications. Thus, it is of interest to see whether the tree structure can be {\em learned}. Specifically, this would mean the following:
	\begin{itemize}
		\item For the first $T$ iterations, use regular ZORO. Compute the gradient estimators $\hat{\mathbf{g}}_{k}$ and let $S_{k} = \text{supp}(\hat{\mathbf{g}}_k)$.
		\item Assume that there exists an underlying tree $\mathcal{T}$, such that all gradients for this problem are $\mathcal{T}$-sparse. For now lets assume that all gradients should be $s$-tree-sparse ({\em i.e.} have $s$ non-zero entries that form a subtree of $\mathcal{T}$).
		\item Observe that each $S_{k}$ is the set of vertices of an $s$-vertex subtree of $\mathcal{T}$. Interestingly, observe that $S_{k}$ alone does not tell us any parent/child relationships.
		\item Let's write $\mathcal{T} = (\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ represents the vertices and $\mathcal{E}$ represents the edges. Lets assume that $T$ is large enough that $\cup_{k=1}^{T} S_k = \mathcal{V}$. 
		\item Here is the interesting question: Can we infer $\mathcal{E}$ from $\{S_1,\ldots, S_{T}\}$? This seems hard, but one could exploit the tree structure. For example if $S_{k_1}$ and $S_{k_2}$ differ by one index, call it $i$, then we can infer that $i$ cannot be a parent to any of the vertices in $S_{k_1}$ or $S_{k_2}$. 
	\end{itemize}
	Suppose that this problem can be solved, and now let $T$ be the minimum number such that the problem is solvable with $S_1,\ldots, S_{T}$. Then one can make an adaptive algorithm that runs regular ZORO for $T$ iterations, then reconstructs $\mathcal{T}$, and then switches to tree-sparse ZORO for the remainder of the optimization process.\\
	
\item Possible applications. We (i.e. HanQin and Daniel) have some expertise in adversarial attacks on classification algorithms. This is quite a trendy application of zeroth-order optimization, so I think it could be a nice way to apply structure-aware ZORO. Three possible ways to do this (and don't worry if this doesn't all make sense to you at this stage):
	\begin{itemize}
		\item Attacks on decision trees. This paper: \cite{cheng2018query} details a way to do this. They use a generic zeroth-order algorithm to do the attack, but it can probably be sped up by using a tree-sparsity-aware zeroth-order algorithm. The downside is that one would need to know the tree, which would mean it's only partially black-box.
		\item Attacks that are sparse in the wavelet domain on Convolutional Neural Networks for image classification.
		\item Block-sparse attacks on Convolutional Neural Networks for image classification.This paper: \cite{xu2018structured} has some details on this, but I haven't read it closely.
	\end{itemize} 

\end{itemize}

\section{Exploiting Block Sparsity for High Dimensional ZORO}

\begin{itemize}
	\item Suppose that we are solving $\text{minimize}_{x\in\mathbb{R}^{d}} f(x)$ where $d$ is extremely large.
	\item ZORO generates $z_{i}\in\mathbb{R}^{d}$ with $z_{i,j} = \pm 1$ and then uses finite differences to get $y_{i} \approx z_{i}^{\top}\mathbf{g}_k$. 
	\item As we've discussed, the $z_i$ are then stacked into a matrix $Z\in\mathbb{R}^{m\times d}$. By construction, this matrix is now full ({\em i.e.} not sparse. When $d$ is large this can require too much memory, and operations with this matrix might be too slow.
	\item So, my idea is the following:
		\begin{itemize}
			\item Divide $\{1,\ldots, d\}$ into $D$ blocks of size $d/D$. Lets label the blocks $\mathbf{b}_{1},\ldots, \mathbf{b}_{D}$. Note that $\mathbf{b}_{a} = \{a*(d/D)+1,\ldots, (a+1)*(d/D)\}$. 
			\item Now, only generate $z_{i}$ that are {\em supported on only one block}. That is, $\text{supp}(z_i) = \mathbf{b}_{a_i}$ for some $a_i$. There are several ways one could do it. Given $i$ one could choose the $a_i$ uniformly at random. Alternatively, at the $k$-th iteration of the gradient descent part of ZORO, one could fix a block $\mathbf{b}_{a_{k}}$, and then choose all the $z_i$ at this iteration to have support $\mathbf{b}_{a_k}$. This turns ZORO into a sort of block coordinate descent algorithm, which could be very interesting to analyze.
			\item That's where this paper: \cite{chun2020uniform} comes in. Hopefully we can use their analysis to establish when the matrix $Z$ has the RIP, and hence use the analysis in the ZORO paper to bound $\|\mathbf{g}_k - \hat{\mathbf{g}}_k\|$. 
		
		\end{itemize}

\end{itemize}

\section{Variance Reduction for ZORO}

Yuchen's note sent on Friday 12th June summarizes this part quite nicely. The only thing I would add is that I think we can further exploit the structure of the error terms, $e_i = z_i^{\top}\nabla^{2}f(x)z_i$. In particular, one can use the Hanson-Wright inequality to get bounds of the form:
$$
\mathbb{P}\left[\left| z_i^{\top}\nabla^{2}f(x)z_i - \mathbb{E}\left[z_i^{\top}\nabla^{2}f(x)z_i\right]\right| \geq t \right] \leq e^{-ct} 
$$
Of course one would then also need to bound the difference between the true mean, $\mathbb{E}\left[z_i^{\top}\nabla^{2}f(x)z_i\right]$, and the sample mean:
$$
\bar{e} = \frac{1}{m}\sum_i e_i = \frac{1}{m} \sum_i z_i^{\top}\nabla^{2}f(x)z_i 
$$
so getting sharper bounds than those given by Popoviciu's inequality might not be so straightforward.
\bibliographystyle{alpha}
\bibliography{CSST2020.bib}
\end{document}